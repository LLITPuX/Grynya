# Специфікація проекту "Міграція memory-manager на MCP"

## 1. Коротке РЕЗЮМЕ (Executive Summary)
Ми інтегруємо існуючу навичку `memory-manager` (та її протоколи запису) з нативним сервером Model Context Protocol (MCP) замість застарілого скрипта `memory_bridge.py`. Це дозволить агенту безпосередньо звертатись до бази нативним інструментарієм без накладних витрат на контейнеризацію (`docker exec`) та копіювання файлів.

## 2. Опис Проблеми
Наразі управління пам'яттю реалізоване через збирання великого JSON-об'єкта, що записується у тимчасовий файл і передається через термінал до `memory_bridge.py`. Цей процес ускладнює подальше розширення логіки графа й агентських систем і не відповідає найкращим практикам сучасних AI-агентів.

## 3. Критерії Успіху
- Агент самостійно зберігає сесії виключно через нативні MCP-викліки.
- Зникла залежність від генерації тимчасових `payload.json` файлів (для `memory_bridge.py`).
- Схема та зв'язки графа залишаються повністю узгодженими з поточною реалізацією (`grynya-schema.md`).

## 4. Портрети Користувачів (User Personas)
- **AI Агент (Гриня):** Використовуватиме нові MCP Tools для збереження кожного кроку розмови.
- **Розробник / Архітектор (USER):** Отримуватиме задоволення від чистої архітектури графа без "костьилів".

## 5. Шлях Користувача (User Journey Flow)
Покроковий шлях взаємодії агента під час викликів `/db`, `/sa` та `/ss` зміниться. Наприклад, для `/sa`:
1. Агент викликає інструмент для створення вузла `Feedback` та зв'язування його з хронологією.
2. Агент викликає інструмент для створення вузла `Analysis` та його прив'язки.
3. Агент викликає інструмент для створення вузла `Response` та його прив'язки.
4. Агент самостійно аналізує контекст, витягує всі розглянуті сутності (Entities) та їхні зв'язки, після чого передає цей сформований список до нового пакетного інструмента (наприклад, `batch_add_entities`), який виконує їх запис за один раз.

## 6. Функціональні Вимоги
### Обов'язково зробити (P0 - Must Have)
- **Вимога 1:** Написання пакетних інструментів на стороні `main.py` для однотипних сутностей та зв'язків (`add_nodes_batch`, `link_nodes_batch`), щоб не "забивати" контекстне вікно моделі поодинокими запитами.
- **Вимога 2:** Рефакторинг інструкцій у `protocol_db.md`, `protocol_sa.md`, `protocol_ss.md` для використання нових MCP Tools.
### Бажано зробити (P1 - Should Have)
- **Вимога 3:** Автоматичне оновлення `LAST_EVENT` та створення `NEXT`-зв'язків під капотом сервера (там, де це можливо спростити).
### Було б чудово (P2 - Nice to Have)
- **Вимога 4:** Механізм відкату (rollback), якщо серія MCP-команд під час одного `/sa` обірвалася посередині виконання.

## 7. Технічна Архітектура
### Модель Даних (Data Model)
Залишається без змін (відповідно до `grynya-schema.md`).
### Компоненти Системи
- **FastMCP Server** (`main.py`): сервер надає інструменти базі даних FalkorDB. Зараз він працює в Docker-контейнері і обробляє запити автономно.
- **AI Агент**: Спілкується з сервером через протокол STDIO (вбудований в IDE).
### Інтеграції (External APIs)
Більше ніяких `docker exec` і Bash-скриптів.

## 8. Нефункціональні Вимоги
- **Продуктивність:** Затримка (latency) вважається не критичною. Зосередженість робиться на зменшенні надмірного використання контексту (Context Window Overhead) за рахунок використання пакетних операторів (Batching) для однотипних сутностей.
- **Масштабування:** Перехід на архітектуру інструментів (Tools) дозволить в майбутньому легко підключати нові навички (skills) до графа, спираючись на готові ендпоїнти.

## 9. Що "Поза рамками" (Out of Scope)
- Зміна самої `grynya-schema.md` або міграція даних (додавання нових об'єктів пам'яті вирішуватиметься в наступній ітерації).

## 10. Відкриті питання для опрацювання розробниками
- Як саме передавати `date` / `time` під час запису `batch`-сутностей? (Можливо, сервер повинен проставляти час створення автоматично за замовчуванням).

## Додаток: Результати Дослідження
- Під час сесії проведено тестування серверу в реальному часі. Інструменти `mcp_falkordb_query_graph` та `mcp_falkordb_add_node` працюють, запитують базу і додають вузли (тестовий вузол було успішно створено і видалено). 
- Сервер дійсно працює, з'єднання `stdio` з Docker налаштоване коректно. Розривів не спостерігається.

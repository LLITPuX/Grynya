# Специфікація проекту: LLM Provider MCP (Grynya-Agent-Box)

## 1. Коротке РЕЗЮМЕ (Executive Summary)
Створення ізольованого у Docker-контейнері Python-сервісу, який виступає як MCP-сервер для зовнішніх клієнтів (надаючи універсальний інструмент для текстових запитів) та як MCP-клієнт для взаємодії з іншими інструментами (наприклад, базою даних FalkorDB). Цей "Агент у коробці" дозволяє викликати різні мовні моделі (Gemini, OpenAI) для виконання атомарних завдань і є фундаментом для майбутньої динамічної маршрутизації промптів безпосередньо з графа знань.

## 2. Опис Проблеми
Поточні субагенти (наприклад, для діагностики графа) або жорстко зашиті в код, або не мають гнучкого, безпечного середовища для виконання. Потрібен універсальний "движок", здатний приймати довільні завдання, підтягувати власні системні промпти та використовувати інструменти екосистеми (на кшталт FalkorDB) без необхідності переписувати клієнтську частину для кожного нового типу субагента.

## 3. Критерії Успіху
- **Універсальність:** Сервер розгортається локально чи на VPS за допомогою простого `docker-compose up`.
- **Сумісність:** Повна відповідність стандарту Model Context Protocol (MCP).
- **Виконуваність:** Здатність передати на вхід текстовий запит (та опціонально системний промпт/контекст) і отримати готовий текст від LLM, яка сама (всередині себе) може попередньо опитати FalkorDB.
- **Стабільність:** Кожен запит виконується ізольовано (stateless-підхід).

## 4. Портрети Користувачів (User Personas)
*   **Гриня (Головний Агент):** Делегує атомарні завдання (наприклад, "прочитай граф і знайди сиріт") на виконання підлеглому "агенту в коробці".
*   **Розробник:** Керує інфраструктурою, додає нові ключі в `.env` або розширює перелік підтримуваних провайдерів (в майбутньому Anthropic, Local LLMs).

## 5. Шлях Користувача (User Journey Flow)
1.  **Ініціація:** Клієнт (Гриня або оркестратор) звертається до інструменту MCP (наприклад, `run_agent_task`).
2.  **Передача контексту:** На вхід подається `user_prompt` (напр., "Проведи діагностику"), опціонально `system_prompt` або `context_nodes` (вказівники на інформацію).
3.  **Виконання (Внутрішній цикл):**
    *   Python-обгортка формує запит до обраного LLM провайдера (за замовчуванням Gemini CLI або OpenAI).
    *   Внутрішній клієнт підключає власні інструменти (напр. доступ до графа).
    *   LLM виконує запит, можливо, в кілька кроків (напр. робить Cypher-запит, аналізує результат).
4.  **Повернення результату:** Повна і фінальна текстова відповідь повертається клієнту єдиним повідомленням. Контекст сесії знищується (Stateless).

## 6. Функціональні Вимоги

### Обов'язково зробити (P0 - Must Have)
- [ ] Окремий Docker-контейнер з Python-виконавцем.
- [ ] Реалізація MCP-сервера (наприклад, через `mcp` SDK або `FastMCP`).
- [ ] Інструмент: `run_agent_task(prompt: str, system_prompt: str = None, context: list = None, model="gemini-2.5-flash-thinking-exp")`
- [ ] Інтеграція з Google Gemini API (через `google-genai`).
- [ ] Інтеграція з OpenAI API (через офіційний SDK).
- [ ] Читання ключів доступу виключно з локального файлу `.env` (`GEMINI_API_KEY`, `OPENAI_API_KEY`).
- [ ] Внутрішня здатність працювати як MCP-клієнт до `cloudrun` та `falkordb` серверів під час обробки LLM.

### Бажано зробити (P1 - Should Have)
- [ ] Логування виконання (що пішло LLM, що вона повернула) у консоль контейнера для зручного дебагу.
- [ ] Обробка помилок (Rate Limits, неправильні ключі) з поверненням зрозумілих повідомлень клієнту.

### Було б чудово (P2 - Nice to Have)
- [ ] Читання списку доступних моделей безпосередньо через API провайдера (dynamic model discovery).
- [ ] Можливість динамічно підтягувати `system_prompt` з графа FalkorDB (якщо в графі є вузол з інструкцією).

## 7. Технічна Архітектура
### Модель Даних (Data Model)
Stateless. Не передбачається збереження розмов чи створення таблиць/шлюзів всередині контейнера. Кожен виклик обробляється "з нуля" з переданим контекстом.

### Компоненти Системи
*   `mcp-server`: Точка входу, що розголошує інструмент `run_agent_task`.
*   `llm-client`: Обгортка, яка трансклює єдиний формат запитів у формат конкретного провайдера (Gemini або OpenAI).
*   `tool-proxy`: Модуль, що дозволяє внутрішній LLM звертатися до інших MCP серверів локальної екосистеми під час роздумів.

### Інтеграції (External APIs)
*   Google GenAI API
*   OpenAI API
*   Локальні MCP сервери (FalkorDB).

### Модель Безпеки та Авторизації
Авторизація перед API-провайдерами відбувається через статичні ключі, вмонтовані в `.env`. Оскільки сервіс планується як внутрішній мікросервіс (в межах Docker-мережі), авторизація клієнтських запитів до цього MCP-сервера наразі не потрібна (довіряємо всім в локальній мережі/пайплайну).

## 8. Нефункціональні Вимоги
- **Продуктивність:** Мінімальний overhead на проксіювання. Час відповіді повністю залежить від API провайдера.
- **Масштабування:** Сервіс має легко масштабуватись (збільшення реплік у Docker) без ризику втрати стану.
- **Надійність:** Якщо один провайдер або модель "падає", сервіс повинен продовжувати працювати (дозволити клієнту повторити запит з іншою моделлю).

## 9. Що "Поза рамками" (Out of Scope)
- Веб-інтерфейс (UI) для чату з ботом.
- Створення та запис логів сесії у FalkorDB (це робитиме `memory-manager` агента, який викликає цей сервіс, а не сам провайдер).
- Інтеграція з Telegram чи іншими месенджерами (залишається виключно як системний MCP-мікросервіс).

## 10. Відкриті питання для опрацювання розробниками
*   Як найкраще реалізувати "вкладеність" MCP? (Коли MCP-сервер стає клієнтом до іншого MCP-сервера під час виконання Tool Call). Необхідно дослідити життєвий цикл сесії клієнта всередині обробника інструменту.
*   Як ефективно прокидати всі доступні інструменти (Tools) зі "світу" (графа, команд) безпосередньо в "мозок" внутрішньої LLM за один виклик.

---
*Специфікацію створено в результаті Discovery Interview (Сесія: session_2026-02-25_12-50-00).*
